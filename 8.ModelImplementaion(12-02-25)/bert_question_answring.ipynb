{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\dell\\anaconda3\\lib\\site-packages (5.24.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from plotly) (24.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\dell\\anaconda3\\lib\\site-packages\\huggingface_hub-0.28.1-py3.8.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertForQuestionAnswering,\n",
    "    BertTokenizerFast,\n",
    ")\n",
    "from scipy.special import softmax\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"deepset/bert-base-cased-squad2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer=BertTokenizerFast.from_pretrained(model_name)\n",
    "model=BertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context = \"\"\"Subhash Chandra Bose passed through quarters inhabited by Englishmen and also met a\n",
    " large number of them in the tram cars. The British using these cars were purposely rude and offensive to Indians in various ways. The sensitive mind of Subhash revolted against such insulting and rude behavior of the British.\n",
    " On many occasions, there was an exchange of hot words between him and misbehaving British.\n",
    "   Majority of the students of the Presidency College, where he studied, were free thinkers. \n",
    "   The college continued to be a storm centre and was looked upon by the British Government \"as a hotbed of sedition, rendezvous of revolutionaries\" and was frequently searched by the police.\n",
    " The first two years of his life were greatly influenced by the group, which styled itself as the neo-Vivekananda group and Subhash developed intellectually during this period. The group generally\n",
    "   followed the teachings of Rama Krishna and Vivekananda with special emphasis on social service as\n",
    " means of spiritual development and was non-aligned to a revolutionary group. \n",
    " The shock of the Great World War roused his political consciousness. He graduated at the age of 22 and enrolled himself for the postgraduate with experimental psychology as a special subject. \n",
    "His father, however, wanted him to go to England to appear for the Indian Civil Services. \n",
    "In spite of his mental reservations, Subhash took it as a challenge. In England, \n",
    "he was greatly impressed with the freedom allowed to students at Cambridge. Every student behaved in a dignified manner. Notwithstanding his preoccupation with his studies,\n",
    " he displayed his public spirit and fearlessness throughout his stay in England. He and K. L. Gouba were selected by the Indian Majlis, to represent the British Government the difficulties the Indian students encountered for admission to the University Officers' Training Corps. Though he took a harsh view of the British high handedness and racial arrogance, \n",
    " he did admire their qualities which exacted him. He himself behaved there in a dignified way and was of the view that Indians who go abroad, must consider themselves to be unofficial ambassadors of the country, who should uphold their country's prestige. \n",
    " He was quite serious in purpose and disliked anybody wasting time on trivialities.\"\"\"\n",
    "question = \"Who were selected by the Indian Majlis?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(question,context,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2627,  1127,  2700,  1118,  1103,  1890, 18285,  6137,   136,\n",
       "           102, 12859, 16481,  1324, 17595, 24880,  2085,  1194,  7541,  9375,\n",
       "          1118,  1483,  2354,  1105,  1145,  1899,   170,  1415,  1295,  1104,\n",
       "          1172,  1107,  1103, 14172,  3079,   119,  1109,  1418,  1606,  1292,\n",
       "          3079,  1127,  3007,  1193, 14708,  1105,  5810,  1106,  5888,  1107,\n",
       "          1672,  3242,   119,  1109,  7246,  1713,  1104, 12859, 16481,  1324,\n",
       "         11733,  1174,  1222,  1216, 27296,  1105, 14708,  4658,  1104,  1103,\n",
       "          1418,   119,  1212,  1242,  6070,   117,  1175,  1108,  1126,  3670,\n",
       "          1104,  2633,  1734,  1206,  1140,  1105,  1940,  1116,  3962, 22300,\n",
       "          1418,   119, 23670,  1104,  1103,  1651,  1104,  1103, 18037,  1531,\n",
       "           117,  1187,  1119,  2376,   117,  1127,  1714,  1341,  1468,   119,\n",
       "          1109,  2134,  1598,  1106,  1129,   170,  4162,  2642,  1105,  1108,\n",
       "          1350,  1852,  1118,  1103,  1418,  2384,   107,  1112,   170,  2633,\n",
       "          4774,  1104, 14516, 14669,   117,  1231, 11131,  1584, 11944,  1104,\n",
       "          8011,  5927,   107,  1105,  1108,  3933,  8703,  1118,  1103,  2021,\n",
       "           119,  1109,  1148,  1160,  1201,  1104,  1117,  1297,  1127,  5958,\n",
       "          4401,  1118,  1103,  1372,   117,  1134, 14174,  2111,  1112,  1103,\n",
       "         15242,   118,   159,  2109,  8752, 16724,  1372,  1105, 12859, 16481,\n",
       "          1324,  1872,  8066,  1193,  1219,  1142,  1669,   119,  1109,  1372,\n",
       "          2412,  1723,  1103, 12815,  1104, 15078, 10892,  1105,   159,  2109,\n",
       "          8752, 16724,  1114,  1957,  7569,  1113,  1934,  1555,  1112,  2086,\n",
       "          1104,  6170,  1718,  1105,  1108,  1664,   118, 14006,  1106,   170,\n",
       "          8953,  1372,   119,  1109,  4900,  1104,  1103,  2038,  1291,  1414,\n",
       "           187, 17237,  1181,  1117,  1741,  8418,   119,  1124,  3024,  1120,\n",
       "          1103,  1425,  1104,  1659,  1105,  7945,  1471,  1111,  1103, 19932,\n",
       "          1114,  6700,  8874,  1112,   170,  1957,  2548,   119,  1230,  1401,\n",
       "           117,  1649,   117,  1458,  1140,  1106,  1301,  1106,  1652,  1106,\n",
       "          2845,  1111,  1103,  1890,  3145,  4326,   119,  1130,  8438,  1104,\n",
       "          1117,  4910, 20624,   117, 12859, 16481,  1324,  1261,  1122,  1112,\n",
       "           170,  4506,   119,  1130,  1652,   117,  1119,  1108,  5958,  7351,\n",
       "          1114,  1103,  4438,  2148,  1106,  1651,  1120,  3900,   119,  4081,\n",
       "          2377, 18492,  1181,  1107,   170, 11902,  2605,  8971,  4758,   119,\n",
       "          1753, 22922, 17277,  1117,  3073, 13335, 18637,  1891,  1114,  1117,\n",
       "          2527,   117,  1119,  6361,  1117,  1470,  4840,  1105,  2945, 18984,\n",
       "          2032,  1117,  2215,  1107,  1652,   119,  1124,  1105,   148,   119,\n",
       "           149,   119,  3414, 24672,  1127,  2700,  1118,  1103,  1890, 18285,\n",
       "          6137,   117,  1106,  4248,  1103,  1418,  2384,  1103,  7866,  1103,\n",
       "          1890,  1651,  8181,  1111, 10296,  1106,  1103,  1239, 13236,   112,\n",
       "          5513,  3158,   119,  3473,  1119,  1261,   170,  8213,  2458,  1104,\n",
       "          1103,  1418,  1344,  3541,  1757,  1105,  5209,   170, 27457,   117,\n",
       "          1119,  1225, 21937,  1147, 11793,  1134,  6129,  1174,  1140,   119,\n",
       "          1124,  1471, 18492,  1181,  1175,  1107,   170, 11902,  2605,  8971,\n",
       "          1236,  1105,  1108,  1104,  1103,  2458,  1115,  5888,  1150,  1301,\n",
       "          6629,   117,  1538,  4615,  2310,  1106,  1129, 12695,  9088,  1116,\n",
       "          1104,  1103,  1583,   117,  1150,  1431,  1146,  8678,  1147,  1583,\n",
       "           112,   188, 19846,   119,  1124,  1108,  2385,  3021,  1107,  3007,\n",
       "          1105, 20604, 11183, 20930,  1159,  1113, 23594,  4233,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output=model(**inputs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_score,end_score=softmax(output.start_logits)[0],softmax(output.end_logits)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = np.argmax(start_score)\n",
    "end_idx = np.argmax(end_score)\n",
    "confidence_score = (start_score[start_idx] + end_score[end_idx]) /2\n",
    "answer_ids = inputs.input_ids[0][start_idx: end_idx + 1]\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n",
    "answer = tokenizer.convert_tokens_to_string(answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K. L. Gouba 0.9873538017272949\n"
     ]
    }
   ],
   "source": [
    "if answer != tokenizer.cls_token:\n",
    "     print(answer, confidence_score)\n",
    "else:\n",
    "     print(None, confidence_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who were selected by the Indian Majlis?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
